{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Labeler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Snorkel Session and Load Data\n",
    "Creates a snorkel session on SQLite database and loads tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "Exercise = candidate_subclass('Exercise', ['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "docs = []\n",
    "distinct_docs = []\n",
    "json_file = 'data/tweets.json'\n",
    "\n",
    "tweets = pd.read_json(json_file)\n",
    "for t in tweets['tweet']:\n",
    "    tweet = ast.literal_eval(t)\n",
    "    #docs.append(tweet['text'])\n",
    "    docs.append(' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet['text']).split()))    \n",
    "\n",
    "train_set = set()\n",
    "dev_set = set()\n",
    "test_set = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    if doc not in distinct_docs:\n",
    "        distinct_docs.append(doc)\n",
    "        if i % 10 == 8:\n",
    "            dev_set.add(doc)\n",
    "        elif i % 10 == 9:\n",
    "            test_set.add(doc)\n",
    "        else:\n",
    "            train_set.add(doc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import CandidateExtractor\n",
    "cand_extractor = CandidateExtractor(Exercise, [], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "('Number of candidates:', 7756)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "('Number of candidates:', 965)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "('Number of candidates:', 964)\n",
      "CPU times: user 2.61 s, sys: 244 ms, total: 2.85 s\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([train_set, dev_set, test_set]):    \n",
    "    cand_extractor.apply(docs, split=i)\n",
    "    print(\"Number of candidates:\", session.query(Exercise).filter(Exercise.split == i).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 965\n",
      "AnnotatorLabels created: 964\n",
      "CPU times: user 24.4 s, sys: 100 ms, total: 24.5 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "# Load Gold Labels\n",
    "from util import load_external_labels\n",
    "%time missed = load_external_labels(session, Exercise, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((965, 1), (964, 1))\n"
     ]
    }
   ],
   "source": [
    "# Load existing dev and test sets\n",
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_gold_test = load_gold_labels(session, annotator_name='gold', split=2)\n",
    "print(L_gold_dev.shape, L_gold_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions (LFs)\n",
    "LF is a python function that accepts a tweet and returns 1 if it marks it as true, -1 if false, or 0 to abstain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks for a kb phrase in the tweet\n",
    "#kb = 'data/kb.txt'\n",
    "            \n",
    "#def LF_distant_supervision(c):   \n",
    "    #with open(kb) as f:\n",
    "        #for phrase in f:\n",
    "            #return 1 if c.find(phrase.strip()) >= 0 else -1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use weak classifier\n",
    "#import classifier\n",
    "\n",
    "#def LF_weak_classifier(c):\n",
    "    #return classify(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random LF\n",
    "import random\n",
    "\n",
    "def LF_random_lf(c):\n",
    "    return random.choice([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another random LF\n",
    "def LF_another_random_lf(c):\n",
    "    return random.choice([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "print(LF_random_lf('a'), LF_another_random_lf('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group LFs in a list for later use\n",
    "#LFs = [LF_distant_supervision, LF_weak_classifier, LF_random_lf, LF_another_random_lf]\n",
    "LFs = [LF_random_lf, LF_another_random_lf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number labeled:', 321)\n"
     ]
    }
   ],
   "source": [
    "# Check size of dev set labeled as exercise tweets using LF_random_lf\n",
    "labeled = []\n",
    "for c in session.query(Exercise).filter(Exercise.split == 1).all():\n",
    "    if LF_random_lf(c) == 1:\n",
    "        labeled.append(c)\n",
    "print(\"Number labeled:\", len(labeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xc but this version of numpy is 0xb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xc but this version of numpy is 0xb"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/redae/anaconda/envs/py2Env/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.497\n",
      "Neg. class accuracy: 0.518\n",
      "Precision            0.265\n",
      "Recall               0.497\n",
      "F1                   0.346\n",
      "----------------------------------------\n",
      "TP: 79 | FP: 219 | TN: 235 | FN: 80\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of LF_random_lf on dev set\n",
    "from snorkel.lf_helpers import test_LF\n",
    "tp, fp, tn, fn = test_LF(session, LF_random_lf, split=1, annotator_name='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LFs\n",
    "from snorkel.annotations import LabelAnnotator\n",
    "labeler = LabelAnnotator(lfs=LFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "CPU times: user 6.63 s, sys: 48 ms, total: 6.68 s\n",
      "Wall time: 6.81 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<7756x2 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 10311 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run labeler\n",
    "import numpy as np\n",
    "np.random.seed(1701)\n",
    "%time L_train = labeler.apply(split=0)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 120 ms, sys: 4 ms, total: 124 ms\n",
      "Wall time: 124 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<7756x2 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 10311 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Load the labels as a sparse matrix\n",
    "%time L_train = labeler.load_matrix(session, split=0)\n",
    "L_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_random_lf</th>\n",
       "      <td>0</td>\n",
       "      <td>0.666194</td>\n",
       "      <td>0.445591</td>\n",
       "      <td>0.222151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_another_random_lf</th>\n",
       "      <td>1</td>\n",
       "      <td>0.663228</td>\n",
       "      <td>0.445591</td>\n",
       "      <td>0.222151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      j  Coverage  Overlaps  Conflicts\n",
       "LF_random_lf          0  0.666194  0.445591   0.222151\n",
       "LF_another_random_lf  1  0.663228  0.445591   0.222151"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  View statistics about the resulting label matrix\n",
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Generative Model\n",
    "Train a model of the LFs to estimate their accuracies and then combine the outputs of the LFs into a noise-aware training labels set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
